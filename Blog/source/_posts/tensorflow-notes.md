---
title: tensorflow_notes
date: 2021-03-30 08:33:42
tags:
- TensorFlow
- AI
---



TensorFlow笔记

<!--more-->



[[_TOC_]]



**神经网络**

优化问题：梯度下降、误差方程

TensorFlow中的数据以张量tensor的形式存在

回归（定量）、分类（定性）



mnist：

[Link](https://mofanpy.com/tutorials/machine-learning/tensorflow/classifier/)

- 准备mnist数据库, 28*28=748
- 搭建网络：addlayer，只有输入层和输出层，输入数据是784个特征，输出是10个特征；激励采用softmax函数
- loss函数：交叉熵函数，用于衡量预测值和真实值的区别，为0是最好的
- 训练方法：梯度下降



过拟合：机器过度自信

解决方法：

- 增加数据量
- 随机dropout， 减少一些节点的影响



CNN**卷积神经网络**

每一层神经网络都有输入值和输出值，输入值是图片的时候，输入实际上是一个二维的数字数组

`卷积`：卷积也就是说神经网络**不再是对每个像素的输入信息做处理了,而是图片上每一小块像素区域进行处理**, 这种做法加强了图片信息的连续性. 使得神经网络能看到图形, 而非一个点. 这种做法同时也加深了神经网络对图片的理解。

- 卷积神经网络有一个批量过滤器, 持续不断的在图片上滚动收集图片里的信息,每一次收集的时候都只是收集一小块像素区域, 然后把收集来的信息进行整理, 这时候整理出来的信息有了一些实际上的呈现, 比如这时的神经网络能看到一些边缘的图片信息, 然后在以同样的步骤, 用类似的批量过滤器扫过产生的这些边缘信息, 神经网络从这些边缘信息里面总结出更高层的信息结构,比如说总结的边缘能够画出眼睛,鼻子等等. 再经过一次过滤, 脸部的信息也从这些眼睛鼻子的信息中被总结出来. 最后我们再把这些信息套入几层普通的全连接神经层进行分类, 这样就能得到输入的图片能被分为哪一类的结果了.



`池化`：在每一次卷积的时候, 神经层可能会无意地丢失一些信息. 这时, 池化 (pooling) 就可以很好地解决这一问题. 而且池化是一个筛选过滤的过程, 能将 layer 中有用的信息筛选出来, 给下一个层分析. 同时也减轻了神经网络的计算负担 ([具体细节参考](http://cs231n.github.io/convolutional-networks/#pool)). 也就是说在卷集的时候, 我们不压缩长宽, 尽量地保留更多信息, 压缩的工作就交给池化了



主流的神经网络：

比较流行的一种搭建结构是这样, 从下到上的顺序, 首先是输入的图片(image), 经过一层卷积层 (convolution), 然后在用池化(pooling)方式处理卷积的信息, 这里使用的是 max pooling 的方式. 然后在经过一次同样的处理, 把得到的第二次处理的信息传入两层全连接的神经层 (fully connected),这也是一般的两层神经网络层,最后在接上一个分类器(classifier)进行分类预测.

[CNN训练数字识别Link](https://mofanpy.com/tutorials/machine-learning/tensorflow/CNN3/)

- 导入mnist数据集
- 定义weight和biase
- 定义卷积：`tf.nn.conv2d`函数是`tensoflow`里面的二维的卷积函数，`x`是图片的所有参数，`W`是此卷积层的权重，然后定义步长`strides=[1,1,1,1]`值，`strides[0]`和`strides[3]`的两个1是默认值，中间两个1代表padding时在x方向运动一步，y方向运动一步，padding采用的方式是`SAME`。
- 定义pooling：接着定义池化`pooling`，为了得到更多的图片信息，padding时我们选的是一次一步，也就是`strides[1]=strides[2]=1`，这样得到的图片尺寸没有变化，而我们希望压缩一下图片也就是参数能少一些从而减小系统的复杂度，因此我们采用`pooling`来稀疏化参数，也就是卷积神经网络中所谓的下采样层。`pooling`有两种，一种是最大值池化，一种是平均值池化，本例采用的是最大值池化`tf.max_pool()`。池化的核函数大小为2x2，因此`ksize=[1,2,2,1]`，步长为2，因此`strides=[1,2,2,1]`:
- 